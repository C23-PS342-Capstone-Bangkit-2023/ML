{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pwsMdxtP82yh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pathlib\n",
        "import zipfile\n",
        "from shutil import copyfile\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XidpOHQuSgpw",
        "outputId": "f1741cc5-1547-48ef-ea30-fc61131e0d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_zip = '/content/drive/MyDrive/dataset_foto/indonesian_food.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "Enr4AWjDQZKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir makanan_indo"
      ],
      "metadata": {
        "id": "Wi2yhvKN6iM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "root_dir = './makanan_indo'\n",
        "\n",
        "def create_train_val_dirs(root_path):\n",
        "    train_dir = os.path.join(root_path, 'training')\n",
        "    valid_dir = os.path.join(root_path, 'validation')\n",
        "\n",
        "    classes = ['ayam bakar', 'ayam geprek', 'bakso', 'gado gado', 'mie ayam', 'rendang', 'sate', 'sayur asem', 'soto  ayam']\n",
        "    subfolders = ['train', 'valid']\n",
        "\n",
        "    for c in classes:\n",
        "        for sf in subfolders:\n",
        "            class_dir = os.path.join(root_path, sf, c)\n",
        "            os.makedirs(class_dir)"
      ],
      "metadata": {
        "id": "bNolsahs6s4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_train_val_dirs(root_path=root_dir)"
      ],
      "metadata": {
        "id": "ecqMrcYS6vQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "\n",
        "  files = []\n",
        "  for filename in os.listdir(SOURCE_DIR):\n",
        "    file_path = os.path.join(SOURCE_DIR, filename)\n",
        "    if os.path.getsize(file_path) > 0:\n",
        "      files.append(filename)\n",
        "    else:\n",
        "      print(f\"{filename}, filename is zero length, so ignoring.\")\n",
        "\n",
        "  train_data_length = int(len(files) * SPLIT_SIZE)\n",
        "  valid_data_length = int(len(files) - train_data_length)\n",
        "\n",
        "  shuffled_data = random.sample(files, len(files))\n",
        "  train_data = shuffled_data[0:train_data_length]\n",
        "  valid_data = shuffled_data[-valid_data_length:]\n",
        "\n",
        "  for filename in train_data:\n",
        "    source_dir = os.path.join(SOURCE_DIR, filename)\n",
        "    dest_dir = os.path.join(TRAINING_DIR, filename)\n",
        "    copyfile(source_dir, dest_dir)\n",
        "\n",
        "  for filename in valid_data:\n",
        "    source_dir = os.path.join(SOURCE_DIR, filename)\n",
        "    dest_dir = os.path.join(VALIDATION_DIR, filename)\n",
        "    copyfile(source_dir, dest_dir)"
      ],
      "metadata": {
        "id": "hmaM827v6xjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "\n",
        "AYAMBAKAR_SOURCE_DIR = '/content/indonesian_food/ayam bakar'\n",
        "AYAMGEPREK_SOURCE_DIR = '/content/indonesian_food/ayam geprek'\n",
        "BAKSO_SOURCE_DIR = '/content/indonesian_food/bakso'\n",
        "MIEAYAM_SOURCE_DIR ='/content/indonesian_food/mie ayam'\n",
        "GADOGADO_SOURCE_DIR = '/content/indonesian_food/gado gado'\n",
        "RENDANG_SOURCE_DIR = '/content/indonesian_food/rendang'\n",
        "SATE_SOURCE_DIR = '/content/indonesian_food/sate'\n",
        "SAYURASEM_SOURCE_DIR = '/content/indonesian_food/sayur asem'\n",
        "SOTOAYAM_SOURCE_DIR = '/content/indonesian_food/soto ayam'\n",
        "\n",
        "\n",
        "TRAINING_DIR = \"/content/makanan_indo/train\"\n",
        "VALIDATION_DIR = \"/content/makanan_indo/valid\"\n",
        "\n",
        "categories = ['ayam bakar', 'ayam geprek', 'bakso', 'gado gado', 'mie ayam', 'rendang', 'sate', 'sayur asem', 'soto  ayam']\n",
        "\n",
        "for category in categories:\n",
        "    training_category_dir = os.path.join(TRAINING_DIR, category)\n",
        "    validation_category_dir = os.path.join(VALIDATION_DIR, category)\n",
        "    if len(os.listdir(training_category_dir)) > 0:\n",
        "        for file in os.scandir(training_category_dir):\n",
        "            os.remove(file.path)\n",
        "    if len(os.listdir(validation_category_dir)) > 0:\n",
        "        for file in os.scandir(validation_category_dir):\n",
        "            os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = 0.8\n",
        "\n",
        "# Run the function for each category\n",
        "for category in categories:\n",
        "    if category == 'ayam bakar':\n",
        "      category_source_dir = AYAMBAKAR_SOURCE_DIR\n",
        "    elif category == 'ayam geprek':\n",
        "      category_source_dir = AYAMGEPREK_SOURCE_DIR\n",
        "    elif category == 'bakso':\n",
        "      category_source_dir = BAKSO_SOURCE_DIR\n",
        "    elif category == 'gado gado':\n",
        "      category_source_dir = GADOGADO_SOURCE_DIR\n",
        "    elif category == 'mie ayam':\n",
        "      category_source_dir = MIEAYAM_SOURCE_DIR\n",
        "    elif category == 'rendang':\n",
        "      category_source_dir = RENDANG_SOURCE_DIR \n",
        "    elif category == 'sate':\n",
        "      category_source_dir = SATE_SOURCE_DIR\n",
        "    elif category == 'sayur asem':\n",
        "      category_source_dir = SAYURASEM_SOURCE_DIR\n",
        "    elif category == 'soto ayam':\n",
        "      category_source_dir = SOTOAYAM_SOURCE_DIR\n",
        "    training_category_dir = os.path.join(TRAINING_DIR, category)\n",
        "    validation_category_dir = os.path.join(VALIDATION_DIR, category)\n",
        "    split_data(category_source_dir, training_category_dir, validation_category_dir, split_size)\n"
      ],
      "metadata": {
        "id": "7gykuW06615R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageGenerator:\n",
        "    def __init__(self, train_dir, validation_dir):\n",
        "        self.train_dir = train_dir\n",
        "        self.validation_dir = validation_dir\n",
        "\n",
        "    def create_generators(self):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1./255.,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "\n",
        "        test_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            self.train_dir,\n",
        "            batch_size=20,\n",
        "            class_mode='categorical',\n",
        "            target_size=(224, 224)\n",
        "        )\n",
        "\n",
        "        validation_generator = test_datagen.flow_from_directory(\n",
        "            self.validation_dir,\n",
        "            batch_size=20,\n",
        "            class_mode='categorical',\n",
        "            target_size=(224, 224)\n",
        "        )\n",
        "\n",
        "        return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "CVA2cHJCOIAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.999):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "metadata": {
        "id": "UN20ASIKOw5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = TRAINING_DIR\n",
        "valid_dir = VALIDATION_DIR \n",
        "callbacks = myCallback()\n",
        "image_gen = ImageGenerator(train_dir, valid_dir)\n",
        "train_generator, validation_generator = image_gen.create_generators()\n",
        "\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation = 'relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(9, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer = Adam(learning_rate = 0.003),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 20,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "uHi3YJamOd03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e10f0e6-b483-4419-9f4f-99648848d519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1433 images belonging to 9 classes.\n",
            "Found 359 images belonging to 9 classes.\n",
            "Epoch 1/20\n",
            "72/72 [==============================] - 54s 716ms/step - loss: 1.7716 - accuracy: 0.3678 - val_loss: 0.8100 - val_accuracy: 0.7409\n",
            "Epoch 2/20\n",
            "72/72 [==============================] - 51s 705ms/step - loss: 1.1666 - accuracy: 0.5387 - val_loss: 0.7034 - val_accuracy: 0.7437\n",
            "Epoch 3/20\n",
            "72/72 [==============================] - 50s 697ms/step - loss: 1.0072 - accuracy: 0.6127 - val_loss: 0.5705 - val_accuracy: 0.7716\n",
            "Epoch 4/20\n",
            "72/72 [==============================] - 49s 687ms/step - loss: 0.9287 - accuracy: 0.6274 - val_loss: 0.5865 - val_accuracy: 0.7521\n",
            "Epoch 5/20\n",
            "72/72 [==============================] - 51s 704ms/step - loss: 0.8752 - accuracy: 0.6525 - val_loss: 0.6010 - val_accuracy: 0.7465\n",
            "Epoch 6/20\n",
            "72/72 [==============================] - 49s 686ms/step - loss: 0.8348 - accuracy: 0.6595 - val_loss: 0.5827 - val_accuracy: 0.7772\n",
            "Epoch 7/20\n",
            "72/72 [==============================] - 51s 713ms/step - loss: 0.7968 - accuracy: 0.6553 - val_loss: 0.5887 - val_accuracy: 0.7716\n",
            "Epoch 8/20\n",
            "72/72 [==============================] - 52s 725ms/step - loss: 0.8137 - accuracy: 0.6609 - val_loss: 0.5799 - val_accuracy: 0.7688\n",
            "Epoch 9/20\n",
            "72/72 [==============================] - 51s 716ms/step - loss: 0.7745 - accuracy: 0.6909 - val_loss: 0.5492 - val_accuracy: 0.7799\n",
            "Epoch 10/20\n",
            "72/72 [==============================] - 51s 704ms/step - loss: 0.7219 - accuracy: 0.6978 - val_loss: 0.5918 - val_accuracy: 0.7493\n",
            "Epoch 11/20\n",
            "72/72 [==============================] - 50s 698ms/step - loss: 0.6928 - accuracy: 0.6943 - val_loss: 0.5485 - val_accuracy: 0.7772\n",
            "Epoch 12/20\n",
            "72/72 [==============================] - 49s 689ms/step - loss: 0.7061 - accuracy: 0.7020 - val_loss: 0.5717 - val_accuracy: 0.7688\n",
            "Epoch 13/20\n",
            "72/72 [==============================] - 53s 734ms/step - loss: 0.7089 - accuracy: 0.7048 - val_loss: 0.5577 - val_accuracy: 0.7827\n",
            "Epoch 14/20\n",
            "72/72 [==============================] - 52s 722ms/step - loss: 0.7100 - accuracy: 0.6930 - val_loss: 0.5929 - val_accuracy: 0.7799\n",
            "Epoch 15/20\n",
            "72/72 [==============================] - 48s 671ms/step - loss: 0.7702 - accuracy: 0.6790 - val_loss: 0.5797 - val_accuracy: 0.7716\n",
            "Epoch 16/20\n",
            "72/72 [==============================] - 51s 715ms/step - loss: 0.7353 - accuracy: 0.6846 - val_loss: 0.5197 - val_accuracy: 0.7744\n",
            "Epoch 17/20\n",
            "72/72 [==============================] - 53s 731ms/step - loss: 0.6612 - accuracy: 0.7111 - val_loss: 0.5762 - val_accuracy: 0.7521\n",
            "Epoch 18/20\n",
            "72/72 [==============================] - 49s 691ms/step - loss: 0.6993 - accuracy: 0.6957 - val_loss: 0.5646 - val_accuracy: 0.7549\n",
            "Epoch 19/20\n",
            "72/72 [==============================] - 50s 706ms/step - loss: 0.7308 - accuracy: 0.6776 - val_loss: 0.5302 - val_accuracy: 0.7799\n",
            "Epoch 20/20\n",
            "72/72 [==============================] - 49s 681ms/step - loss: 0.7227 - accuracy: 0.6853 - val_loss: 0.5549 - val_accuracy: 0.7716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "class_names = ['ayam bakar', 'ayam geprek', 'bakso', 'gado gado', 'mie ayam', 'rendang', 'sate', 'sayur asem', 'soto  ayam']  \n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    # Predicting images\n",
        "    path = '/content/' + fn\n",
        "    img = load_img(path, target_size=(224, 224))\n",
        "  \n",
        "    x = img_to_array(img)\n",
        "    x /= 255\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    images = np.vstack([x])\n",
        "  \n",
        "    classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "    predicted_class_index = np.argmax(classes[0])  \n",
        "  \n",
        "    predicted_class = class_names[predicted_class_index]  \n",
        "  \n",
        "    print(fn + \" is a \" + predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "bo7tud_MZkdJ",
        "outputId": "84e7e101-ad3e-4b53-9cfa-701b53e12537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78725b17-5c7c-4c1d-879c-23fdb259ead2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78725b17-5c7c-4c1d-879c-23fdb259ead2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving rendang (59).jpg to rendang (59).jpg\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "rendang (59).jpg is a rendang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = 'saved_model/1'\n",
        "\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "metadata": {
        "id": "pdfBvLEFdPUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"Speed\" \n",
        "export_dir = 'saved_model/1'\n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_file = pathlib.Path('./model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "id": "d6TWtIoSOvbH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}